# Databricks notebook source


# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC
# MAGIC # Data Science with Databricks
# MAGIC
# MAGIC ## ML is key to wind turbine farm optimization
# MAGIC
# MAGIC The current market makes energy even more strategic than before. Being able to ingest and analyze our Wind turbine state is a first step, but this isn't enough to thrive in a very competitive market.
# MAGIC
# MAGIC We need to go further to optimize our energy production, reduce maintenance cost and reduce downtime. Modern data company achieve this with AI.
# MAGIC
# MAGIC <style>
# MAGIC .right_box{
# MAGIC   margin: 30px; box-shadow: 10px -10px #CCC; width:650px;height:300px; background-color: #1b3139ff; box-shadow:  0 0 10px  rgba(0,0,0,0.6);
# MAGIC   border-radius:25px;font-size: 35px; float: left; padding: 20px; color: #f9f7f4; }
# MAGIC .badge {
# MAGIC   clear: left; float: left; height: 30px; width: 30px;  display: table-cell; vertical-align: middle; border-radius: 50%; background: #fcba33ff; text-align: center; color: white; margin-right: 10px}
# MAGIC .badge_b { 
# MAGIC   height: 35px}
# MAGIC </style>
# MAGIC <link href='https://fonts.googleapis.com/css?family=DM Sans' rel='stylesheet'>
# MAGIC <div style="font-family: 'DM Sans'; display: flex; align-items: flex-start;">
# MAGIC   <!-- Left Section -->
# MAGIC   <div style="width: 50%; color: #1b3139; padding-right: 20px;">
# MAGIC     <div style="color: #ff5f46; font-size:80px;">90%</div>
# MAGIC     <div style="font-size:30px; margin-top: -20px; line-height: 30px;">
# MAGIC       Enterprise applications will be AI-augmented by 2025 —IDC
# MAGIC     </div>
# MAGIC     <div style="color: #ff5f46; font-size:80px;">$10T+</div>
# MAGIC     <div style="font-size:30px; margin-top: -20px; line-height: 30px;">
# MAGIC        Projected business value creation by AI in 2030 —PWC
# MAGIC     </div>
# MAGIC   </div>
# MAGIC
# MAGIC   <!-- Right Section -->
# MAGIC   <div class="right_box", style="width: 50%; color: red; font-size: 30px; line-height: 1.5; padding-left: 20px;">
# MAGIC     But—huge challenges getting ML to work at scale!<br/><br/>
# MAGIC     In fact, most ML projects still fail before getting to production
# MAGIC   </div>
# MAGIC </div>
# MAGIC
# MAGIC ## Machine learning is data + transforms.
# MAGIC
# MAGIC ML is hard because delivering value to business lines isn't only about building a Model. <br>
# MAGIC The ML lifecycle is made of data pipelines: Data-preprocessing, feature engineering, training, inference, monitoring and retraining...<br>
# MAGIC Stepping back, all pipelines are data + code.
# MAGIC
# MAGIC
# MAGIC <img style="float: right; margin-top: 10px" width="500px" src="https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/team_flow_marc.png" />
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/marc.png" style="float: left;" width="80px"> 
# MAGIC <h3 style="padding: 10px 0px 0px 5px">Marc, as a Data Scientist, needs a data + ML platform accelerating all the ML & DS steps:</h3>
# MAGIC
# MAGIC <div style="font-size: 19px; margin-left: 73px; clear: left">
# MAGIC <div class="badge_b"><div class="badge">1</div> Build Data Pipeline supporting real time (with DLT)</div>
# MAGIC <div class="badge_b"><div class="badge">2</div> Data Exploration</div>
# MAGIC <div class="badge_b"><div class="badge">3</div> Feature creation</div>
# MAGIC <div class="badge_b"><div class="badge">4</div> Build & train model</div>
# MAGIC <div class="badge_b"><div class="badge">5</div> Deploy Model (Batch or serverless realtime)</div>
# MAGIC <div class="badge_b"><div class="badge">6</div> Monitoring</div>
# MAGIC </div>
# MAGIC
# MAGIC **Marc needs a Data Intelligence Platform**. Let's see how we can deploy a Predictive Maintenance model in production with Databricks.

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC # Single click deployment with AutoML
# MAGIC
# MAGIC Let's see how we can now leverage data from out operational systems to build out time and cost budget prediction models
# MAGIC
# MAGIC Our first step as Data Scientist is to analyze and build the features we'll use to train our model.
# MAGIC
# MAGIC The tables we will leverage have been saved within our Delta Live Table pipeline. All we have to do is read this information, analyze it and start an Auto-ML run.
# MAGIC
# MAGIC <img src="https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-ds-flow.png" width="1000px">
# MAGIC
# MAGIC *Note: Make sure you switched to the "Machine Learning" persona on the top left menu.*
# MAGIC
# MAGIC
# MAGIC <!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->
# MAGIC <img width="1px" src="https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F04-Data-Science-ML%2F04.1-automl-iot-turbine-predictive-maintenance&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F04-Data-Science-ML%2F04.1-automl-iot-turbine-predictive-maintenance&version=1">

# COMMAND ----------

# MAGIC %pip install --quiet databricks-sdk==0.40.0 databricks-feature-engineering==0.8.0 mlflow==2.20.1
# MAGIC dbutils.library.restartPython()


root_path=dbutils.widgets.get("root_path")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Data exploration and analysis
# MAGIC
# MAGIC Let's review our dataset and start analyze the data we have to predict our churn

# COMMAND ----------

# DBTITLE 1,Quick data exploration
import plotly.express as px
from pyspark.sql.functions import collect_list

df = spark.table(f"{{.catalog}}.{{.schema}}.drill_cost_model_gold")
grouped_df = df.groupBy('COST_DESC').agg(
    collect_list('ACTUAL_COST').alias('cost')
)

# Collect the data to the driver
collected_data = grouped_df.collect()

# Process the collected data
for row in collected_data:
    cost = row['cost']
    acct = row['COST_DESC']
    
    fig = px.histogram(cost, nbins=20, title=f'Histogram for {acct} ')
    fig.update_layout(xaxis_title='cost', yaxis_title='Count')
    fig.show()

# COMMAND ----------

# MAGIC %md As we can see in these graph, we can clearly see some anomaly on the readings we get from sensor F. Let's continue our exploration and use the std we computed in our main feature table

# COMMAND ----------

# MAGIC %md
# MAGIC ### Further data analysis and preparation using pandas API
# MAGIC
# MAGIC Because our Data Scientist team is familiar with Pandas, we'll use `pandas on spark` to scale `pandas` code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.
# MAGIC
# MAGIC Typicaly Data Science project would involve more advanced preparation and likely require extra data prep step, including more complex feature preparation. We'll keep it simple for this demo.
# MAGIC
# MAGIC *Note: Starting from `spark 3.2`, koalas is builtin and we can get an Pandas Dataframe using `pandas_api()`.*

# COMMAND ----------

# DBTITLE 1,Custom pandas transformation / code on top of your entire dataset (koalas)
 
cost_dataset=spark.table(f"{{.catalog}}.{{.schema}}.drill_cost_model_gold")
 # Convert to pandas (koalas)
dataset = cost_dataset.pandas_api()

# Select the columns we would like to use as ML Model features. #Note: we removed percentiles_sensor_A/B/C.. feature to make the demo easier
columns = [
    "API_NUMBER",
    "PRODUCING_FORMATION",
    "GEO_RISK_INDEX",
    "DAYS_FROM_SPUD",
    "TOTAL_DEPTH",
    "COST_DESC",
    "ACTUAL_COST"
]
dataset = dataset[columns]

# Drop missing values
dataset = dataset.dropna()   
display(dataset)

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC
# MAGIC ## Write to Feature Store 
# MAGIC
# MAGIC <img src="https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/mlops-end2end-flow-feature-store.png" style="float:right" width="500" />
# MAGIC
# MAGIC Once our features are ready, we'll save them in Databricks Feature Store. Under the hood, features store are backed by a Delta Lake table.
# MAGIC
# MAGIC This will allow discoverability and reusability of our feature accross our organization, increasing team efficiency.
# MAGIC
# MAGIC Feature store will bring traceability and governance in our deployment, knowing which model is dependent of which set of features. It also simplify realtime serving.
# MAGIC
# MAGIC Make sure you're using the "Machine Learning" menu to have access to your feature store using the UI.

# COMMAND ----------

# MAGIC %md
# MAGIC #### COST MODEL **TABLE**

# COMMAND ----------


from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup

fe = FeatureEngineeringClient()
try:
  #drop table if exists
  spark.sql('drop table if exists drilling_cost_features')
  fe.drop_table(name=f'{{.catalog}}.{{.schema}}.drilling_cost_features')
except:
  pass
#Note: You might need to delete the FS table using the UI
job_phase_feature_table = fe.create_table(
  name=f'{{.catalog}}.{{.schema}}.drilling_cost_features',
  primary_keys=['API_NUMBER','COST_DESC'],
  schema=dataset.spark.schema(),
  description='These features are derived from the drilling_cost table in the data intelligence platform.  We made some basic transformations and removed NA value.'
)

fe.write_table(df=dataset.drop_duplicates(subset=['API_NUMBER','COST_DESC']).to_spark(), name=f'{{.catalog}}.{{.schema}}.drilling_cost_features')
features = fe.read_table(name=f'{{.catalog}}.{{.schema}}.drilling_cost_features')
display(features)

# COMMAND ----------

# MAGIC %md-sandbox
# MAGIC
# MAGIC ## Accelerating Predictive maintenance model creation using MLFlow and Databricks Auto-ML
# MAGIC
# MAGIC MLflow is an open source project allowing model tracking, packaging and deployment. Everytime your datascientist team work on a model, Databricks will track all the parameter and data used and will save it. This ensure ML traceability and reproductibility, making it easy to know which model was build using which parameters/data.
# MAGIC
# MAGIC ### A glass-box solution that empowers data teams without taking away control
# MAGIC
# MAGIC While Databricks simplify model deployment and governance (MLOps) with MLFlow, bootstraping new ML projects can still be long and inefficient. 
# MAGIC
# MAGIC Instead of creating the same boilerplate for each new project, Databricks Auto-ML can automatically generate state of the art models for Classifications, regression, and forecast.
# MAGIC
# MAGIC
# MAGIC <img width="1000" src="https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png"/>
# MAGIC
# MAGIC
# MAGIC Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks of efforts.
# MAGIC
# MAGIC <br style="clear: both">
# MAGIC
# MAGIC <img style="float: right" width="600" src="https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-auto-ml.png"/>
# MAGIC
# MAGIC ### Using Databricks Auto ML with our Churn dataset
# MAGIC
# MAGIC Auto ML is available in the "Machine Learning" space. All we have to do is start a new Auto-ML experimentation and select the feature table we just created.
# MAGIC
# MAGIC
# MAGIC
# MAGIC While this often is done using the UI, you can also leverage the [python API](https://docs.databricks.com/applications/machine-learning/automl.html#automl-python-api-1)

# COMMAND ----------

# MAGIC %md
# MAGIC #### COST MODEL

# COMMAND ----------

from datetime import datetime

# decleare input valriables for automl run
xp_path = f'{root_path}/experiments/drilling/job_cost'
xp_name = f"automl_drilling_cost_model_{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}"
dbutils.fs.mkdirs(xp_path)




training_dataset = fe.read_table(name=f'{{.catalog}}.{{.schema}}.drilling_cost_features').drop('API_NUMBER')


try:
    from databricks import automl
    automl_run = automl.regress(
        experiment_name = xp_name,
        experiment_dir = xp_path,
        dataset = training_dataset,
        target_col = "ACTUAL_COST",
        timeout_minutes = 5
    )
except Exception as e:
    if "cannot import name 'automl'" in str(e):
        # Note: cannot import name 'automl' from 'databricks' likely means you're using serverless. Dbdemos doesn't support autoML serverless API - this will be improved soon.
        # Adding a temporary workaround to make sure it works well for now - ignore this for classic run
        raise e
    else:
        raise e

# COMMAND ----------

# MAGIC %md
# MAGIC AutoML saved our best model in the MLFlow registry. Open the experiment from the AutoML run to explore its artifact and analyze the parameters used, including traceability to the notebook used for its creation.
# MAGIC
# MAGIC If we're ready, we can move this model into Production stage in a click, or using the API. Let' register the model to Unity Catalog and move it to production.
# MAGIC
# MAGIC You can programatically get the last best run from your automl training:
# MAGIC ```
# MAGIC from mlflow import MlflowClient
# MAGIC
# MAGIC # retrieve best model trial run
# MAGIC trial_id = automl_run.best_trial.mlflow_run_id
# MAGIC model_uri = "runs:/{}/model".format(automl_run.best_trial.mlflow_run_id)
# MAGIC #Use Databricks Unity Catalog to save our model
# MAGIC latest_model = mlflow.register_model(model_uri, f"{{.catalog}}.{db}.{model_name}")
# MAGIC # Flag it as Production ready using UC Aliases
# MAGIC MlflowClient().set_registered_model_alias(name=f"{{.catalog}}.{db}.{model_name}", alias="prod", version=latest_model.version)
# MAGIC ```

# COMMAND ----------

from mlflow import MlflowClient
import mlflow

model_name = 'drilling_cost_model'
full_model_name = f"{{.catalog}}.{{.schema}}.{model_name}"

# Set the registry URI to Unity Catalog
mlflow.set_registry_uri("databricks-uc")

# Retrieve best model trial run
trial_id = automl_run.best_trial.mlflow_run_id
model_uri = f"runs:/{trial_id}/model"

# Use Databricks Unity Catalog to save our model
latest_model = mlflow.register_model(model_uri, full_model_name)

# Flag it as Production ready using UC Aliases
MlflowClient().set_registered_model_alias(name=full_model_name, alias="prod", version=latest_model.version)

# COMMAND ----------

# MAGIC
# MAGIC %md-sandbox
# MAGIC
# MAGIC ## Realtime model serving with Databricks serverless serving
# MAGIC
# MAGIC <img style="float: right; margin-left: 20px" width="800" src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/ep_model_serving_creation.gif?raw=true" />
# MAGIC
# MAGIC Databricks also provides serverless serving.
# MAGIC
# MAGIC Click on model Serving, enable realtime serverless and your endpoint will be created, providing serving over REST api within a Click.
# MAGIC
# MAGIC Databricks Serverless offer autoscaling, including downscaling to zero when you don't have any traffic to offer best-in-class TCO while keeping low-latencies model serving.
# MAGIC

# COMMAND ----------

from mlflow.deployments import get_deploy_client
import time
MODEL_SERVING_ENDPOINT_NAME= "drilling-cost-endpoint"
client = get_deploy_client("databricks")
try:
    endpoint = client.create_endpoint(
        name=MODEL_SERVING_ENDPOINT_NAME,
        config={
            "served_entities": [
                {
                    "name": "drilling-cost-endpoint",
                    "entity_name": f"{{.catalog}}.{{.schema}}.{model_name}",
                    "entity_version": latest_model.version,
                    "workload_size": "Small",
                    "scale_to_zero_enabled": True
                }
            ]
        }
    )
except Exception as e:
    if "already exists" in str(e):
        print(f"Endpoint {{.catalog}}.{{.schema}}.{MODEL_SERVING_ENDPOINT_NAME} already exists. Skipping creation.")
    else:
        raise e

while client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['config_update'] == 'IN_PROGRESS':
    time.sleep(10)
