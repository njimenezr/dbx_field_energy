# Databricks notebook source
# MAGIC %md
# MAGIC # Predictive Maintenance Inference - Batch or serverless real-time
# MAGIC
# MAGIC
# MAGIC With AutoML, our best model was automatically saved in our MLFlow registry.
# MAGIC
# MAGIC All we need to do now is use this model to run Inferences. A simple solution is to share the model name to our Data Engineering team and they'll be able to call this model within the pipeline they maintained. That's what we did in our Delta Live Table pipeline!
# MAGIC
# MAGIC Alternatively, this can be schedule in a separate job. Here is an example to show you how MLFlow can be directly used to retriver the model and run inferences.
# MAGIC
# MAGIC *Make sure you run the previous notebook to be able to access the data.*
# MAGIC
# MAGIC ## Environment Recreation
# MAGIC The cell below downloads the model artifacts associated with your model in the remote registry, which include `conda.yaml` and `requirements.txt` files. In this notebook, `pip` is used to reinstall dependencies by default.
# MAGIC
# MAGIC <!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->
# MAGIC <img width="1px" src="https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=1444828305810485&notebook=%2F04-Data-Science-ML%2F04.3-running-inference-iot-turbine&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F04-Data-Science-ML%2F04.3-running-inference-iot-turbine&version=1">

# COMMAND ----------

# MAGIC %pip install mlflow==2.20.1 databricks-sdk==0.40.0

# COMMAND ----------


# Use the catalog and create the main schema
spark.sql(f"use catalog {{.catalog}}")

# Use the schema 
spark.sql(f"use schema {{.schema}}")

# COMMAND ----------

import os
import mlflow
from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository

mlflow.set_registry_uri('databricks-uc')
local_path = ModelsArtifactRepository(f"models:/{{.catalog}}.{{.schema}}.drilling_cost_model@prod").download_artifacts("") # download model from remote registry

requirements_path = os.path.join(local_path, "requirements.txt")
if not os.path.exists(requirements_path):
  dbutils.fs.put("file:" + requirements_path, "", True)

# COMMAND ----------

# MAGIC %pip install -r $requirements_path
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %md
# MAGIC ##Deploying the model for batch inferences
# MAGIC
# MAGIC <img style="float: right; margin-left: 20px" width="800" src="https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/ep_model_serving_creation.gif?raw=true" />
# MAGIC
# MAGIC Now that our model is available in the Registry, we can load it to compute our inferences and save them in a table to start building dashboards.
# MAGIC
# MAGIC We will use MLFlow function to load a pyspark UDF and distribute our inference in the entire cluster. If the data is small, we can also load the model with plain python and use a pandas Dataframe.
# MAGIC
# MAGIC If you don't know how to start, Databricks can generate a batch inference notebook in just one click from the model registry: Open MLFlow model registry and click the "User model for inference" button!

# COMMAND ----------

# MAGIC %md
# MAGIC ### Scaling inferences using Spark 
# MAGIC We'll first see how it can be loaded as a spark UDF and called directly in a SQL function:

# COMMAND ----------

import mlflow
mlflow.set_registry_uri('databricks-uc')

predict_cost = mlflow.pyfunc.spark_udf(spark, f"models:/{{.catalog}}.{{.schema}}.drilling_cost_model@prod", result_type="float")
#We can use the function in SQL
spark.udf.register("predict_cost", predict_cost)

# COMMAND ----------

columns = predict_cost.metadata.get_input_schema().input_names()
spark.table('og_demo_workshop.drilling.drill_cost_model_gold').withColumn("predicted_cost", predict_cost(*columns)).display()

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT
# MAGIC   API_NUMBER,
# MAGIC   COST_DESC,
# MAGIC   ROUND(predict_cost(
# MAGIC     PRODUCING_FORMATION, DAYS_FROM_SPUD, TOTAL_DEPTH, COST_DESC
# MAGIC   ),0) as prediction
# MAGIC FROM
# MAGIC   {{.catalog}}.{{.schema}}.drill_cost_model_gold

# COMMAND ----------

# MAGIC %md
# MAGIC ## Real time model inference
# MAGIC
# MAGIC Let's now deploy our model behind a realtime model serving endpoint.
# MAGIC
# MAGIC We'll then use this endpoint in our GenAI Agentic demo to be able to fetch a turbine status in realtime
# MAGIC

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC You can now view the status of the Feature Serving Endpoint in the table on the **Serving endpoints** page. Click **Serving** in the sidebar to display the page.

# COMMAND ----------

from mlflow import deployments

def score_model(dataset):
    client = mlflow.deployments.get_deploy_client("databricks")
    predictions = client.predict(
        endpoint='drilling-job-time-endpoint',
        inputs={"dataframe_split": dataset.to_dict(orient='split')}
    )
    return predictions

# Select only the required columns for the model
required_columns = ['PRODUCING_FORMATION', 'JOB_PHASE', 'JOB_SUB_PHASE', 'JOB_PHASE_DEPTH']
dataset = spark.table(f"{{.catalog}}.{{.schema}}.drill_time_model_gold").select(*required_columns).limit(3).toPandas()

# Deploy your model and uncomment to run your inferences live!
display(score_model(dataset))