# Databricks notebook source


# COMMAND ----------

# MAGIC %md
# MAGIC # 1. Generating Well Operational Datasets
# MAGIC In this notebook, we'll use [Databricks Labs' Data Generator](https://databrickslabs.github.io/dbldatagen/public_docs/index.html) to simulate datasets that might potentially come from our field data capture tool of choice. Our approach is to:
# MAGIC 1. Create catalogs, schemas, and volumes in Unity Catalog (as necessary)
# MAGIC 1. Build Spark DataFrames from data generation *specifications*
# MAGIC 2. Write data to a bronze schema to be consumed by our DLT pipeline in later steps.
# MAGIC
# MAGIC
# MAGIC Let's get started.

# COMMAND ----------

# Install dbldatagen to cluster
%pip install --quiet dbldatagen
dbutils.library.restartPython()

# COMMAND ----------

# Import specific functions from the data_generation_specs module
# These functions are used to generate various types of data for the project
from data_generation_specs import (
    generate_well_header_data,  # Function to generate well header data
    generate_job_phase_data,    # Function to generate job phase data
    generate_npt_data,          # Function to generate non-productive time (NPT) data
    generate_cost_account_data, # Function to generate cost account data
)

# COMMAND ----------

# Imports for data generation

# Importing functions from pyspark.sql module for various SQL operations
from pyspark.sql import functions as F

# Importing specific functions from pyspark.sql.functions module
from pyspark.sql.functions import col, lit, when, date_format, to_date, sum, expr

# Importing datetime, date, and timedelta classes from the datetime module for date manipulations
from datetime import datetime, date, timedelta

# Importing dbldatagen module for data generation
import dbldatagen as dg

# Importing Window class from pyspark.sql.window module for window functions
from pyspark.sql.window import Window

from math import floor


# COMMAND ----------

# MAGIC %md
# MAGIC ## 1.1 Unity Catalog Objects
# MAGIC In this section, we'll set up some objects in Unity Catalog to hold files, data, and connections to external systems (e.g. Azure SQL).
# MAGIC
# MAGIC ***NOTE:** We can create these objects with DBSQL commands or through the Catalog Explorer UI.*

# COMMAND ----------



# Use the catalog and create the main schema
spark.sql(f"use catalog {{.catalog}}")

spark.sql(f"create schema if not exists {{.schema}}")

# Use the schema 
spark.sql(f"use schema {{.schema}}")


# Grant access to the user group. 
# spark.sql(f"grant all privileges on catalog {{.catalog}} to `account users`")
# spark.sql(f"grant select on schema {{.catalog}}.{schema} to `users`")

# COMMAND ----------

# MAGIC %md ##1.2 Generating Data from Specs
# MAGIC We can now generate data from [data generation specs](https://databrickslabs.github.io/dbldatagen/public_docs/generating_column_data.html). These specs are defined in the `data_generation_specs` python module in our `scripts` folder.
# MAGIC
# MAGIC We will generate these datasets:
# MAGIC - Assets (e.g. Pipeline Sections)
# MAGIC - Sensors (e.g. Pressure Sensors, Temperature Sensors, Flow Meters)
# MAGIC - Sensor Readings (e.g. IoT data)

# COMMAND ----------

# MAGIC %md ### 1.2.1 Well Header
# MAGIC
# MAGIC The `generate_well_header_data` function generates a dataset containing metadata about wells. This dataset includes information such as:
# MAGIC
# MAGIC - Well ID
# MAGIC - Well Name
# MAGIC - Location (Latitude, Longitude)
# MAGIC - Depth
# MAGIC - Operator
# MAGIC - Start Date
# MAGIC - End Date
# MAGIC
# MAGIC This data is essential for identifying and categorizing wells in our analysis.

# COMMAND ----------

# Generate well header data and build the DataFrame
wellsdf = generate_well_header_data(spark,200)
wellsdf = wellsdf.build(spark)


# If the well is planned, set COMPLETION_DATE to None, otherwise keep the original value
wellsdf = wellsdf.withColumn("COMPLETION_DATE", 
    F.when(F.col("CURRENT_STATUS") == "Planned", None)
    .otherwise(F.col("COMPLETION_DATE")))

# If the well is planned, set SPUD_DATE to None, otherwise keep the original value
wellsdf = wellsdf.withColumn("SPUD_DATE", 
    F.when(F.col("CURRENT_STATUS") == "Planned", None)
    .otherwise(F.col("SPUD_DATE")))

# If the well is planned, set TOTAL_DEPTH to None, otherwise keep the original value
wellsdf = wellsdf.withColumn("TOTAL_DEPTH", 
    F.when(F.col("CURRENT_STATUS") == "Planned", None)
    .otherwise(F.col("TOTAL_DEPTH")))

# If the well is planned, set SURFACE_CASING_DEPTH to None, otherwise keep the original value
wellsdf = wellsdf.withColumn("SURFACE_CASING_DEPTH", 
    F.when(F.col("CURRENT_STATUS") == "Planned", None)
    .otherwise(F.col("SURFACE_CASING_DEPTH")))


display(wellsdf.sort("API_NUMBER"))


# COMMAND ----------

# MAGIC %sql
# MAGIC -- Drop the table if it exists to ensure a clean slate for the new data
# MAGIC -- This operation is idempotent, meaning it can be run multiple times without causing an error
# MAGIC -- The table being dropped is 'all_wells_bronze' in the 'drilling' schema
# MAGIC DROP TABLE IF EXISTS drilling.all_wells_bronze;

# COMMAND ----------

# Save the DataFrame as a table in the specified catalog and schema
# The saveAsTable method writes the DataFrame to a table
# The table name is constructed using the catalog, schema, and table name 'all_wells_bronze'
# This will create a new table or replace the existing table with the same name
wellsdf.write.saveAsTable(f"{{.catalog}}.{{.schema}}.{'all_wells_bronze'}")

# COMMAND ----------

###################################################################################
# Convert wells DataFrame to a list of dictionaries excluding planned wells
# Create a window specification for the entire DataFrame

# # Select relevant columns from the wells DataFrame
# wellsdf = wellsdf.select("API_NUMBER", "CURRENT_STATUS", "TOTAL_DEPTH", "SPUD_DATE", "SURFACE_CASING_DEPTH","PRODUCING_FORMATION")

# # Filter out rows where the well is planned
# wellsdf = wellsdf.filter(F.col("CURRENT_STATUS") != "Planned")

# Calculate the percentile rank for TOTAL_DEPTH and add it as a new column
wellsdf = wellsdf.withColumn(
    "DEPTH_PERCENTILE_RANK", F.percent_rank().over(Window.orderBy(F.col("TOTAL_DEPTH")))
)

# Display the sorted DataFrame
display(wellsdf.sort("API_NUMBER"))

# Convert the DataFrame to a list of dictionaries
# wellDict = [row.asDict() for row in wellsdf.collect()]

import pandas as pd
pandas_df = wellsdf.toPandas()

# Display the list of dictionaries
test = pandas_df[["API_NUMBER", "CURRENT_STATUS", "TOTAL_DEPTH", "SPUD_DATE", "SURFACE_CASING_DEPTH","PRODUCING_FORMATION","DEPTH_PERCENTILE_RANK","GEO_RISK_INDEX"]]
test=test[test["CURRENT_STATUS"] != "Planned"]

test['API_NUMBER'] = test['API_NUMBER'].astype(int)

test['TOTAL_DEPTH'] = test['TOTAL_DEPTH'].astype(int)
test['SURFACE_CASING_DEPTH'] = test['SURFACE_CASING_DEPTH'].astype(int)
test['DEPTH_PERCENTILE_RANK'] = test['DEPTH_PERCENTILE_RANK'].astype(float)
test['GEO_RISK_INDEX'] = test['GEO_RISK_INDEX'].astype(float)


wellDict = test.to_dict(orient='records')

display(wellDict)

# COMMAND ----------

# MAGIC %md ### 1.2.2 Drilling Job Phase Details
# MAGIC
# MAGIC The `generate_job_phase_data` function generates a dataset containing detailed information about the phases of drilling jobs. This dataset includes:
# MAGIC
# MAGIC - Job ID
# MAGIC - Phase Name
# MAGIC - Start Time
# MAGIC - End Time
# MAGIC - Start Depth
# MAGIC - End Depth
# MAGIC - Phase Description
# MAGIC
# MAGIC This data is crucial for analyzing the efficiency and progress of drilling operations.

# COMMAND ----------

# Generate job phase data using the wellTable list of dictionaries
# The generate_job_phase_data function takes the wellTable as input and returns a DataFrame
jobphasedf = generate_job_phase_data(wellTable=wellDict)

# Display the generated job phase DataFrame
display(jobphasedf)

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Drop the table if it exists to ensure a clean slate for the new data
# MAGIC -- This operation is idempotent, meaning it can be run multiple times without causing an error
# MAGIC -- The table being dropped is 'drilling_job_phase_bronze' in the 'drilling' schema
# MAGIC DROP TABLE IF EXISTS drilling.drilling_job_phase_bronze;

# COMMAND ----------

# Save the DataFrame as a table in the specified catalog and schema
jobphasedf.write.saveAsTable(f"{{.catalog}}.{{.schema}}.{'drilling_job_phase_bronze'}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1.2.3 Non-Productive Time Log
# MAGIC
# MAGIC The `generate_npt_data` function provides a dataframe of Non-Productive Time (NPT) that occurs on a job. This NPT is related to drilling activities.

# COMMAND ----------

# Generate Non-Productive Time (NPT) data using the wellTable list of dictionaries
# The generate_npt_data function takes the Spark session and wellTable as input and returns a DataFrame
npt_data = generate_npt_data(spark, wellTable=wellDict)

# Display the generated NPT DataFrame
display(npt_data)

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Drop the table if it exists to ensure a clean slate for the new data
# MAGIC -- This operation is idempotent, meaning it can be run multiple times without causing an error
# MAGIC -- The table being dropped is 'drilling_npt_bronze' in the 'drilling' schema
# MAGIC DROP TABLE IF EXISTS drilling.drilling_npt_bronze;

# COMMAND ----------

# Save the DataFrame as a table in the specified catalog and schema
npt_data.write.saveAsTable(f"{{.catalog}}.{{.schema}}.{'drilling_npt_bronze'}")

# COMMAND ----------

# MAGIC %md
# MAGIC ### 1.2.4 Cost Detail by Account
# MAGIC
# MAGIC The `generate_cost_account_data` function provides a detailed breakdown of costs by account. This function is essential for tracking and managing expenses related to various activities within a project.

# COMMAND ----------

# Generate cost account data using the wellTable list of dictionaries
# The generate_cost_account_data function takes the wellTable as input and returns a DataFrame
well_cost_data = generate_cost_account_data(wellDict)

# Display the generated cost account DataFrame
display(well_cost_data)

# COMMAND ----------

# MAGIC %sql
# MAGIC -- Drop the table if it exists to ensure a clean slate for the new data
# MAGIC -- This operation is idempotent, meaning it can be run multiple times without causing an error
# MAGIC -- The table being dropped is 'drilling_cost_bronze' in the 'drilling' schema
# MAGIC DROP TABLE IF EXISTS drilling.drilling_cost_bronze;

# COMMAND ----------

# Save the well cost data DataFrame as a table in the specified catalog and schema
# The table name is 'drilling_cost_bronze'
# The catalog and schema variables are used to specify the location of the table
well_cost_data.write.saveAsTable(f"{{.catalog}}.{{.schema}}.{'drilling_cost_bronze'}")
